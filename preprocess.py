# -*- coding: utf-8 -*-
"""preprocess

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-WizjCdllD7M0uHHfnmF0wgP8oTHRL3K
"""


import json
from stanfordcorenlp import StanfordCoreNLP
from nltk.tokenize.treebank import TreebankWordDetokenizer

nlp = StanfordCoreNLP(r'/content/drive/Shared drives/SigmaLaw/stanford-corenlp-full-2018-10-05', quiet=False)
props = {'annotators': 'tokenize, ssplit, pos, lemma, parse, ner, coref, kbp, depparse', 'pipelineLanguage': 'en'}

def build_complete_ner(word, ner, l):
  if(l[0].get('ner') == ner):
    word = word + " " + build_complete_ner(l[0].get("word"), l[0].get('ner'), l[1:len(l)])
  return word

# To update parties based on ner
def NER(sentences):
  new_tokens = []
  replace_dict={}
  ner = {}
    
  for each in sentences:
    q = each.get("tokens")

    for j in range(0,len(q)):
      if(q[j].get("ner") == "PERSON" or q[j].get("ner") == "ORGANIZATION"):
        word = build_complete_ner(q[j].get("word"), q[j].get("ner"), q[j+1 : len(q)])


        #Build the prefix
        if(q[j].get("ner") == "PERSON"):
            prefix = "P"
        else:
            prefix = "O"
        prefix = prefix + str(len(ner.keys())+1) + "$"

        
        if(j==0):
          ner[word] = prefix
          words = word.split(" ")
          for l in words:
            replace_dict[l] = (prefix+l+"$")
            new_tokens.append(prefix + l + "$")
        if(j!=0 and q[j-1].get("ner") != q[j].get("ner")):
          ner[word] = prefix

          words = word.split(" ")
          for l in words:
            replace_dict[l] = (prefix+l)
            new_tokens.append(prefix + l + "$")

      else:
        new_tokens.append(q[j].get("word"))
  return new_tokens, ner

def preprocess(text):
  result = json.loads(nlp.annotate(text, properties=props))
  sentences = result['sentences']
  new_tokens, ner = NER(sentences)
  corefs = result['corefs']
  for i in corefs.values():
    coref = i[0].get('text')
    for j in ner.keys():
      if(j in coref):
        prefix = ner[j]
        for word in i:
          sentNum = word.get('sentNum')
          for current_index in range(word.get('startIndex')-1,word.get('endIndex')-1):
            index = 0
            for k in range(0,sentNum-1):
              index += len(sentences[k].get('tokens'))
            index += current_index
            if(new_tokens[index][2:3] != "$"):
              new_tokens[index] = prefix+new_tokens[index]+"$"
  text = TreebankWordDetokenizer().detokenize(new_tokens)
  return text

